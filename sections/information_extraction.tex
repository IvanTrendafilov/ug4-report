\chapter{Information extraction}
This chapter discusses the implementation of the information extraction component. In Section 5.1, we briefly motivate its functionality. In Section 5.2, we describe some of the challenges of dealing with non-homogeneous input data and also outline our approach to named-entity recognition and relation extraction. 

\section{Overview}
The goal of the information extraction component is to obtain useful information from incoming messages. The main tasks performed by this component are named-entity recognition, email extraction, HTML removal, header parsing, quoted text removal and relation extraction. Because this component is the first processing step after entry into the system, it is equipped with methods to deal with potentially dirty data (5.2.1). Once relevant information is extracted, the result is passed as input to the response generation component and is used to generate a response. For example, the outputs of the named-entity recognition (NER) task might be used to compose a personalized greeting in the beginning of the message -- e.g., ``Hello \emph{John}''.

\section{Implementation}
In order to extract the information we are interested in, we apply a series of transformations to each incoming message. These steps are summarized in the algorithm below:
\begin{enumerate}
\item remove HTML % 1
\vspace{-5mm}
\item remove any quoted messages % 2 
\vspace{-5mm}
\item cleanse headers % 3
\vspace{-5mm}
\item extract \textit{From, Reply-To, Subject, To} headers % 4
\vspace{-5mm}
\item extract message body % 5
\vspace{-5mm}
\item perform NER on the message body % 6
\vspace{-5mm}
\item extract all emails from the message body % 7
\vspace{-5mm}
\item compute any relationships between emails and named-entities %8 
\end{enumerate}

\subsection{Dealing with dirty data}
As we explained in Section 4.3, a web crawler is one of the main sources of new email messages for our system. Whilst this aids automation, it also creates a number of problems. First, because messages are posted by human forum participants, they rarely conform to MIME or any other RFC memoranda. The lack of standardization makes it impossible to rely on a compliant parsers to extract the headers and payload of our messages. Under MIME, the body part of a message is separated from the headers via a blank line [REF]. This is often omitted in human posts. Furthermore, email clients and servers attach custom headers to processed email messages, so there is no standard set of headers we can match against. Finally, some of the messages downloaded by the crawler are not emails at all, so we also need a way to discard them.

We observe that correct headers follow a set pattern: \textit{``Header: content''}. Furthermore, there is at most one header per line and headers are placed on consecutive lines in the message. Using that observation, we propose a best-effort algorithm to address the problem above. We use a vocabulary $V$ of 144 verified headers (compiled ahead of time). Then, for each incoming message $M$, we use a regular expression to match the contents of the message against the standard set of headers $S$, such that $S = $\{$Subject, To, From, Reply$-$To\}$. If we fail to match at least three elements of $S$, we discard the message, as it is not a valid email. If we find a match, we then iterate over all lines in the message to find $L_{vmax}$, defined to be the last line that contains a header $\in V$. $L_{vmax}$ allows us to split the message into two segment -- headers and body. Lastly, we iterate over all lines in the headers segment and match each line against our header heuristic. If there is a match, we augment $V$, which gradually improves the performance of the algorithm.

Another problem with parsing incoming email messages is HTML. Popular web-based email services regularly use HTML to format outgoing messages. As these messages are multi-part, some are encoded in both \emph{text/plain} and \emph{text/html}, whilst others only offer \emph{text/html}. For messages available only in HTML format, we construct a HTML parse tree and remove all nodes that do not contain text. Finally, we strip all tags and convert any ampersand character codes to ASCII, where possible (\emph{\&amp;} to \emph{\&}, etc.)

\subsection{Named-entity recognition}
Named-entity recognition (NER) is an information extraction task which seeks to locate and classify entities in text into a set of predefined categories -- e.g., \textit{Persons, Locations, Organizations}. For our prototype system, we are interested only in the \emph{Persons} category, although other categories might have applications for certain scam variations. By performing NER on the message body, we aim to determine the names of all actors in the scam with high accuracy and pass that information to the response generation component. This is a challenging problem, but there are systems available which offer high performance on these tasks.

One example is the Stanford Named Entity Recognizer [REF]. It is a Java implementation of a linear chain Conditional Random Field (CRF) classifier [REF, REF] and comes with large pre-trained models on the CoNLL 2003, MUC-6, MUC-7 and ACE named entity corpora. In this project we are specifically interested in the supplied 3 class model, as it is reported to have a F$_{1}$ score of 90.36 for the \emph{Persons} class on the CoNNL 2003 dataset. Its features are presented in Table 5.1.

\begin{table}[h]
  \centering
   \scalebox{0.85}{
  \begin{tabular}{l}
  \hline
  \textbf{Feature} \\ \hline
  Current word \\
  Previous word \\
  Next word \\
  Current word char. n-gram \\
  Current POS tag \\
  Surrounding POS tag sequence \\
  Presence of word in left window \\
  Presence of word in right window \\ \hline
  \end{tabular}
  }
   \caption{Features of Stanford NER 3 class model.}
\end{table}

Whilst the aforementioned entity corpora provide a robust coverage of the English language, we have also measured the performance of the model on AFF-specific emails on a small validation set. The results are presented in Section 8.2. 

\subsection{Relation extraction}
Another important problem is to determine the relationship between named entities and extracted email addresses. As we explained in Section 2.1, a single scam message may involve multiple actors and contain multiple email addresses. It is necessary to pair actors with the correct email addresses when composing a reply. Failure to do so may result in replying to a non-existing address or addressing the scammer under the wrong name. Formally, given a set of named entities $N = $\{$n_{1}, n_{2},...,n_{m}$\} and a set of email addresses $E = $\{$e_{1}, e_{2},...,e_{p}$\}, compute all pairs $(n, e)$, such that each pair belongs to a unique actor in the scam. This is a difficult problem, but we can observe that in AFF scams email addresses are often a variation of a person's real name. Alternatively, they are located in proximity to a named entity.

We propose a two-round algorithm. In the first round, we remove all domain names from $\forall e \in E$ and compute string similarity using bigrams $\forall (n, e) \in N, E$. Next, we rank all pairs by similarity score $Sc$, remove the top pairs for each named entity from $N, E$ and place them in a bucket $B$, provided $Sc > minconst$, where $mincost$ is the minimum bigram similarity score required ($minconst = 0.40$ in our implementation). In the second round, we rank all remaining pairs $(n, e)$ by word distance (measured by the number of words between $n$ and $e$). Finally, for each remaining $n \in N$, we add the $(n, e)$ with the lowest word distance score to $B$. The bucket now contains all valid $(n, e)$ pairs.